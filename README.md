# GQSAT 

Can Q-learning with Graph Networks learn a Generalizable Branching Heuristic for a SAT solver?


## Setting up UBUNTU
You will need to run everything in an ubuntu terminal. If you have Windows 10, I recommend using Windows Subsystem for Linux https://ubuntu.com/wsl

## Getting repo
You can either clone this repo or download the files and work locally.

## Installing Required Packages
``Dockerfile`` provided in the original code shows the modules that need to be installed. For me, this is the series of commands I used to install the required packages.

```
sudo apt install python3-pip
python3 -m pip install numpy
pip3 install torch torchvision torchaudio
pip install gym
pip3 install torch-scatter
pip install torch-geometric
pip install torch-sparse
```

## How to add metadata for evaluation
This will add a METADATA file that is necessary for your validation problems in training and your testing problems when testing
* First download your dataset from https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html
* Unzip the dataset and partition the ``*/cnf`` files into training, validation, and/or testing folders
* Run the command below once for your validation folder (if any) and once for your testing folder (if any)
```python3 add_metadata.py --eval-problems-paths <path_to_folder_with_cnf>```

## How to train
This will train GQSAT using your training problems and validate using your validation problems
* First open train.sh in a text editor
* Replace the path in ``--train-problems-paths ./data/uf20-91/train1Problem \`` with the path to your training folder
* Replace the path in ``--eval-problems-paths ./data/uf20-91/validation \`` with the path to your validation folder
```./train.sh```

## How to evaluate 
This will run the learned branching heuristic on your test problems
* First open evaluate.sh in a text editor
* Replace the path in ``--eval-problems-paths ./data/uf250-1065/test \`` with the path to your test folder
* Replace the path in ``--model-dir ./runs/uf20-91-1/ \`` with the path to the folder generated by your training step
* Replace the file in ``--model-checkpoint model_10000.chkp \`` with the model you want to use (I evaluated all the models on the validation problems and chose the one with the highest median_relative_score and then the highest mean_relative_score if there was a tie)
```./evaluate.sh```

## How to change maximum (model) decisions

* Open evaluate.sh in a text editor
* Change then number in ``--test_time_max_decisions_allowed 500 \`` to the number of maximum decisions you want to allocate to GQSAT

## How to change output format of evaluate.sh
We only want to obtain the MRIR values and mean_relative_score values (iteration improvement) for our experimental results.
* Open ``utils.py`` from the gqsat folder
* comment out or change the print statements depending on what you want to output during the evaluation of each problem
* Open ``evaluation.py`` 
* comment out or change the print statements depending on what you want to output and the end of evaluation

## Output evaluate.sh to a file
To make working with the data easier, you may want to pipe the output of evaluate.sh to a file.
* Open ``evaluate.sh``
* Add a line like ``>> ./data/uf250-1065/uf250-1065-gqsat-train1.tsv`` specifying the folder and file to put your output in.

## Some files included in the repo

* The ``data`` folder contains our output from evaluate.sh and from running MiniSat by itself. Each file starts with the name of the dataset tested. Files ending with ``max10`` specify evaluations with 10 maximum decisions for GQSAT. Files ending with ``train1`` specify evaluations run with a model trained on a single problem. Files ending with ``minisat-data`` contain results from only MiniSat.
* The ``runs`` folder contains models generated by trainings on SAT 20-91 with 1 problem (uf20-91-1) and with 800 problems (uf20-91-800). For our experiments we chose model_10000.chkp from uf20-91-1 and model_29000.chkp from uf20-91-800.
* ``miniSatOutputDataScraper.py`` is my own code that parses the MiniSat results of each test problem and stitches them together in one tab separated file.
* ``evaluateV2.py`` is a variation of the original ``evaluate.py`` with slightly different print statements to fit what I wanted for my experiments.

## How plots were generated
We generated plots using Google Sheets. Here is the link to the file https://docs.google.com/spreadsheets/d/1e0TgH0f8kEpV5ZpxRpoYRRJL6Unm36XSqnbpw5BS7Wo/edit?usp=sharing


## Acknowledgements

Here is the link to the original code https://github.com/NVIDIA/GraphQSat 
